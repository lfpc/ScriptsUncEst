{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs and pre-definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Bibliotecas padrões python e utils pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, Normalize\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import random_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Define o computador utilizado como cuda (gpu) se existir ou cpu caso contrário\n",
    "print(torch.cuda.is_available())\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Bibliotecas desenvolvidas\n",
    "\n",
    "https://github.com/lfpc/Uncertainty_Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN_models as models\n",
    "import uncertainty.comparison as unc_comp\n",
    "import uncertainty.quantifications as unc\n",
    "import uncertainty.losses as losses\n",
    "import uncertainty.train_and_eval_with_g as TE_g\n",
    "import NN_utils as utils\n",
    "import NN_utils.train_and_eval as TE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Data download and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "transforms_test = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR10(\n",
    "root=\"data\",\n",
    " train=True,\n",
    " download=True,\n",
    "transform=transforms_train)\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "root=\"data\",\n",
    "train=False,\n",
    "download=True,\n",
    "transform=transforms_test)\n",
    "\n",
    "train_size = int(0.85*len(training_data))\n",
    "val_size = len(training_data) - train_size\n",
    "training_data, validation_data = random_split(training_data, [train_size, val_size])\n",
    "\n",
    "validation_data = copy.deepcopy(validation_data)\n",
    "validation_data.dataset.transform = transforms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size,shuffle = True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size,shuffle = False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "\n",
    "loss_fn = losses.selective_net_2(loss_criterion,const_var = 'g')\n",
    "from torch.nn.functional import softmax\n",
    "def SM_fn(g,lamb = 1, dim = -1):\n",
    "    lamb = torch.as_tensor(lamb)\n",
    "    w = softmax(lamb*g,dim=dim)\n",
    "    return w\n",
    "\n",
    "risk_dict = {'empirical_risk':losses.selective_net_2(loss_criterion,optim_method = None,head = None,alpha = 1),\n",
    "             'bce_risk':lambda x,label: torch.mean(loss_criterion(x[0],label)),\n",
    "             'constraint':lambda x,label: loss_fn.get_constraint(x[1]),\n",
    "             'selective_risk_g':lambda x,label: unc_comp.selective_risk(x,label,unc_type = 'g'),\n",
    "            'selective_risk_mcp':  lambda x,label: unc_comp.selective_risk(x[0],label,unc_type = unc.MCP_unc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 , loss =  2.2514974846559412\n",
      "Epoch  2 , loss =  1.9665232649971458\n",
      "Epoch  3 , loss =  1.8280073193942799\n",
      "Epoch  4 , loss =  1.7078420355740715\n",
      "Epoch  5 , loss =  1.6032123896654915\n",
      "Epoch  6 , loss =  1.5422946141747866\n",
      "Epoch  7 , loss =  1.4964145138684442\n",
      "Epoch  8 , loss =  1.4534417427287383\n",
      "Epoch  9 , loss =  1.423595219219432\n",
      "Epoch  10 , loss =  1.3908122365614948\n",
      "Epoch  11 , loss =  1.365041228462668\n",
      "Epoch  12 , loss =  1.3331127517363603\n",
      "Epoch  13 , loss =  1.3030422014348648\n",
      "Epoch  14 , loss =  1.2799266602011288\n",
      "Epoch  15 , loss =  1.2535001704272102\n",
      "Epoch  16 , loss =  1.228092850376578\n",
      "Epoch  17 , loss =  1.2072189339469461\n",
      "Epoch  18 , loss =  1.1897019809835097\n",
      "Epoch  19 , loss =  1.1701095944292406\n",
      "Epoch  20 , loss =  1.1450455695040085\n",
      "Epoch  21 , loss =  1.1202746826059677\n",
      "Epoch  22 , loss =  1.1072877142008613\n",
      "Epoch  23 , loss =  1.0858445107235628\n",
      "Epoch  24 , loss =  1.0697068792230944\n",
      "Epoch  25 , loss =  1.053233442587011\n",
      "Epoch  26 , loss =  1.0416765527164236\n",
      "Epoch  27 , loss =  1.0273475879781386\n",
      "Epoch  28 , loss =  1.015381746712853\n",
      "Epoch  29 , loss =  0.999139159006231\n",
      "Epoch  30 , loss =  0.989777711279252\n",
      "Epoch  31 , loss =  0.9718117223066443\n",
      "Epoch  32 , loss =  0.971124424794141\n",
      "Epoch  33 , loss =  0.9558775582033046\n",
      "Epoch  34 , loss =  0.940226548279033\n",
      "Epoch  35 , loss =  0.9363651725825142\n",
      "Epoch  36 , loss =  0.9287954000865712\n",
      "Epoch  37 , loss =  0.9217726983743555\n",
      "Epoch  38 , loss =  0.9097581110281103\n",
      "Epoch  39 , loss =  0.8993555112446056\n",
      "Epoch  40 , loss =  0.8928075967115514\n",
      "Epoch  41 , loss =  0.882056386050056\n",
      "Epoch  42 , loss =  0.8769674723288592\n",
      "Epoch  43 , loss =  0.8683433869305779\n",
      "Epoch  44 , loss =  0.8613713138243732\n",
      "Epoch  45 , loss =  0.8560207277185776\n",
      "Epoch  46 , loss =  0.8460370271346148\n",
      "Epoch  47 , loss =  0.8428368954097524\n",
      "Epoch  48 , loss =  0.8359422882865457\n",
      "Epoch  49 , loss =  0.8294019729950849\n",
      "Epoch  50 , loss =  0.822743250762715\n",
      "Epoch  51 , loss =  0.8124362952568952\n",
      "Epoch  52 , loss =  0.8095167531686671\n",
      "Epoch  53 , loss =  0.8061740314259248\n",
      "Epoch  54 , loss =  0.7954006213300369\n",
      "Epoch  55 , loss =  0.7922535879471723\n",
      "Epoch  56 , loss =  0.7911972942071802\n",
      "Epoch  57 , loss =  0.7872507107959074\n",
      "Epoch  58 , loss =  0.7784954484070049\n",
      "Epoch  59 , loss =  0.7694101595878601\n",
      "Epoch  60 , loss =  0.7648514217488906\n",
      "Epoch  61 , loss =  0.7638962295476128\n",
      "Epoch  62 , loss =  0.7607056790940901\n",
      "Epoch  63 , loss =  0.753447465616114\n",
      "Epoch  64 , loss =  0.7480005888377919\n",
      "Epoch  65 , loss =  0.7369919936095967\n",
      "Epoch  66 , loss =  0.7379902196631712\n",
      "Epoch  67 , loss =  0.7307346075422624\n",
      "Epoch  68 , loss =  0.7319374755550834\n",
      "Epoch  69 , loss =  0.7226427414838006\n",
      "Epoch  70 , loss =  0.7219419765472412\n",
      "Epoch  71 , loss =  0.7178003441586214\n",
      "Epoch  72 , loss =  0.7103491769818698\n",
      "Epoch  73 , loss =  0.7066533608296338\n",
      "Epoch  74 , loss =  0.6994030549946954\n",
      "Epoch  75 , loss =  0.6969612566162559\n",
      "Epoch  76 , loss =  0.7029326384909013\n",
      "Epoch  77 , loss =  0.6889866357691148\n",
      "Epoch  78 , loss =  0.6865434012693518\n",
      "Epoch  79 , loss =  0.68179067085771\n",
      "Epoch  80 , loss =  0.6814434099197387\n",
      "Epoch  81 , loss =  0.6822899760919459\n",
      "Epoch  82 , loss =  0.6759635721234715\n",
      "Epoch  83 , loss =  0.6749827235586503\n",
      "Epoch  84 , loss =  0.6663075649738311\n",
      "Epoch  85 , loss =  0.6577210713134093\n",
      "Epoch  86 , loss =  0.6549430670457728\n",
      "Epoch  87 , loss =  0.6554001026995042\n",
      "Epoch  88 , loss =  0.6533421316567589\n",
      "Epoch  89 , loss =  0.6486268552611856\n",
      "Epoch  90 , loss =  0.6505776185147902\n",
      "Epoch  91 , loss =  0.6464841841950136\n",
      "Epoch  92 , loss =  0.6389449624454274\n",
      "Epoch  93 , loss =  0.6402428846499499\n",
      "Epoch  94 , loss =  0.6340111626597011\n",
      "Epoch  95 , loss =  0.6322250592007357\n",
      "Epoch  96 , loss =  0.6330637684289147\n",
      "Epoch  97 , loss =  0.6333037485795863\n",
      "Epoch  98 , loss =  0.6230721503846786\n",
      "Epoch  99 , loss =  0.6227010370703304\n",
      "Epoch  100 , loss =  0.6155329268820146\n",
      "Epoch  101 , loss =  0.6097728617051068\n",
      "Epoch  102 , loss =  0.6143590967795428\n",
      "Epoch  103 , loss =  0.6065997576012331\n",
      "Epoch  104 , loss =  0.611049778251087\n",
      "Epoch  105 , loss =  0.6090537275286282\n",
      "Epoch  106 , loss =  0.5976532980974982\n",
      "Epoch  107 , loss =  0.598151568735347\n",
      "Epoch  108 , loss =  0.5958026822875527\n"
     ]
    }
   ],
   "source": [
    "#model with auxiliary head 'h' defined paralell to the classifier.\n",
    "#model_h = transform_in_selective_vgg(models.Model_CNN_with_g_and_h(10,blocks = main_layer)).cuda()\n",
    "model_h = models.Model_CNN_with_g_and_h(10).cuda()\n",
    "optimizer = torch.optim.SGD(model_h.parameters(), lr=1e-3,momentum = 0.9)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = losses.selective_net_2(loss_criterion,w_fn = SM_fn,head = model_h.get_h,alpha = 0.5)\n",
    "\n",
    "model_trainer_h = TE_g.Trainer_with_g(model_h,optimizer,loss_fn, train_dataloader,validation_dataloader,c=0.8,risk_dict = risk_dict)\n",
    "model_trainer_h.fit(train_dataloader,1000)\n",
    "#state_dict  = model_h.state_dict()\n",
    "\n",
    "model_h.return_g = False\n",
    "acc = TE.model_acc(model_h,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = TE.model_acc(model_h,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with auxiliary head as the main classifier\n",
    "#model_f = transform_in_selective_vgg(models.Model_CNN_with_g(10,blocks = main_layer)).cuda()\n",
    "model_f = models.Model_CNN_with_g(10).cuda()\n",
    "optimizer = torch.optim.SGD(model_f.parameters(), lr=1e-3,momentum = 0.9)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = losses.selective_net_2(loss_criterion,w_fn = SM_fn,head = 'y',alpha = 0.5)\n",
    "\n",
    "model_trainer_f = TE_g.Trainer_with_g(model_f,optimizer,loss_fn, train_dataloader,validation_dataloader,c=0.8,risk_dict = risk_dict)\n",
    "model_trainer_f.fit(train_dataloader,1000)\n",
    "#state_dict  = model_h.state_dict()\n",
    "\n",
    "model_f.return_g = False\n",
    "acc = TE.model_acc(model_f,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = TE.model_acc(model_f,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with auxiliary head as the main classifier\n",
    "model_nohead = models.Model_CNN_with_g(10).cuda()\n",
    "#model_nohead = transform_in_selective_vgg(models.Model_CNN_with_g(10,blocks = main_layer)).cuda()\n",
    "optimizer = torch.optim.SGD(model_nohead.parameters(), lr=1e-3,momentum = 0.9,weight_decay = 5e-4)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = losses.selective_net_2(loss_criterion,w_fn = SM_fn)\n",
    "\n",
    "model_trainer_nohead = TE_g.Trainer_with_g(model_nohead,optimizer,loss_fn, train_dataloader,validation_dataloader,c=0.8,risk_dict = risk_dict)\n",
    "model_trainer_nohead.fit(train_dataloader,1000)\n",
    "#state_dict  = model_h.state_dict()\n",
    "\n",
    "model_nohead.return_g = False\n",
    "acc = TE.model_acc(model_nohead,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = TE.model_acc(model_nohead,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "PATH = r'/home/luis-felipe/Uncertainty_Estimation/torch_models'\n",
    "PATH_trainer = r'/home/luis-felipe/Uncertainty_Estimation/torch_models/trainer'\n",
    "SUFIX = '_softmax'\n",
    "\n",
    "model_trainer_h.hist_val.loss_criterion = None\n",
    "model_trainer_h.hist_val.risk_dict = None\n",
    "model_trainer_h.hist_train.loss_criterion = None\n",
    "model_trainer_h.hist_train.risk_dict = None\n",
    "\n",
    "model_trainer_f.hist_train.loss_criterion = None\n",
    "model_trainer_f.hist_train.risk_dict = None\n",
    "model_trainer_f.hist_val.loss_criterion = None\n",
    "model_trainer_f.hist_val.risk_dict = None\n",
    "\n",
    "model_trainer_nohead.hist_train.loss_criterion = None\n",
    "model_trainer_nohead.hist_train.risk_dict = None\n",
    "model_trainer_nohead.hist_val.loss_criterion = None\n",
    "model_trainer_nohead.hist_val.risk_dict = None\n",
    "\n",
    "\n",
    "torch.save(model_h.state_dict(), PATH + '/selective_h' + SUFIX)\n",
    "with open(PATH_trainer + r\"/hist_val_h_trainer\"+SUFIX, \"wb\") as output_file:\n",
    "    pickle.dump(model_trainer_h.hist_val,output_file)\n",
    "with open(PATH_trainer + r\"/hist_train_h_trainer\"+SUFIX, \"wb\") as output_file:\n",
    "    pickle.dump(model_trainer_h.hist_train,output_file)\n",
    "    \n",
    "torch.save(model_f.state_dict(), PATH + '/selective_f'+SUFIX)\n",
    "with open(PATH_trainer + r\"/hist_train_f_trainer\"+SUFIX, \"wb\") as output_file:\n",
    "    pickle.dump(model_trainer_f.hist_train,output_file)\n",
    "with open(PATH_trainer + r\"/hist_val_f_trainer\"+SUFIX, \"wb\") as output_file:\n",
    "    pickle.dump(model_trainer_f.hist_val,output_file)\n",
    "    \n",
    "torch.save(model_nohead.state_dict(), PATH + '/selective_nohead'+SUFIX)\n",
    "with open(PATH_trainer + r\"/hist_train_nohead_trainer\"+SUFIX, \"wb\") as output_file:\n",
    "    pickle.dump(model_trainer_nohead.hist_train,output_file)\n",
    "with open(PATH_trainer + r\"/hist_val_nohead_trainer\"+SUFIX, \"wb\") as output_file:\n",
    "    pickle.dump(model_trainer_nohead.hist_val,output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_trainer_h.hist_val.acc_c_mcp,label = 'MCP')\n",
    "plt.plot(model_trainer_h.hist_val.acc_list,label = 'ACC_0')\n",
    "plt.plot(model_trainer_h.hist_val.acc_c_entropy,label = 'Entropy')\n",
    "plt.plot(model_trainer_h.hist_val.acc_c_g, label = 'g')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in risk_dict:\n",
    "    plt.plot(model_trainer_h.hist_val.risk[key],label = key)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for key in risk_dict:\n",
    "    plt.plot(model_trainer_f.hist_val.risk[key],label = key)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
