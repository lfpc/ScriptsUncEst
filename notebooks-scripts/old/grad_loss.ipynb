{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, Normalize\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import random_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o computador utilizado como cuda (gpu) se existir ou cpu caso contrário\n",
    "print(torch.cuda.is_available())\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN_models as models\n",
    "import uncertainty.comparison as unc_comp\n",
    "import uncertainty.quantifications as unc\n",
    "import uncertainty.losses as losses\n",
    "import uncertainty.train_and_eval_with_g as TE_g\n",
    "import NN_utils as utils\n",
    "import NN_utils.train_and_eval as TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "transforms_test = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.CIFAR10(\n",
    "root=\"data\",\n",
    " train=True,\n",
    " download=True,\n",
    "transform=transforms_train)\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "root=\"data\",\n",
    "train=False,\n",
    "download=True,\n",
    "transform=transforms_test)\n",
    "\n",
    "train_size = int(0.95*len(training_data))\n",
    "val_size = len(training_data) - train_size\n",
    "training_data, validation_data = random_split(training_data, [train_size, val_size])\n",
    "\n",
    "validation_data = copy.deepcopy(validation_data)\n",
    "validation_data.dataset.transform = transforms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size,shuffle = True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size,shuffle = False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_grads(grads_1,grads_2):\n",
    "    loss = -torch.dot(grads_1,grads_2)\n",
    "    return loss\n",
    "\n",
    "class loss_grads(torch.nn.Module):\n",
    "    '''Defines LCE loss - Devries(2018)'''\n",
    "    def __init__(self,params,criterion = dot_grads):\n",
    "        super().__init__()\n",
    "        self.criterion = criterion\n",
    "        self.params = params\n",
    " \n",
    "    def forward(self, grads_1,grads_2):\n",
    "        loss_grad = torch.tensor(0.,requires_grad = True)\n",
    "        for i,(n,p) in enumerate(self.params()):\n",
    "            if 'g_layer' in n:\n",
    "                continue\n",
    "            loss_grad = loss_grad + dot_grads(grads_1[i].view(-1),grads_2[i].view(-1))\n",
    "        return loss_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model_CNN_with_g(10).to(dev)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3,momentum = 0.9)\n",
    "\n",
    "loss_criterion_0 = nn.NLLLoss()\n",
    "loss_criterion_1 = lambda x,label: torch.mean(loss_criterion_0(x[0],label))\n",
    "loss_criterion_g = lambda x,label: torch.mean(x[1].view(-1)*loss_criterion_0(x[0],label))\n",
    "\n",
    "loss_fn = loss_grads(model.named_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisar se os retain_graph são necessários e como colocar loss_v e grad_v para fora do loop\n",
    "utils.unfreeze_params(model)\n",
    "model.train()\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for image,label in train_dataloader:\n",
    "        image,label = image.to(dev), label.to(dev)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss_t = loss_criterion_1(output,label)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    for image,label in train_dataloader:\n",
    "        image,label = image.to(dev), label.to(dev)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "\n",
    "        loss_t = loss_criterion_g(output,label)\n",
    "        grads_t = torch.autograd.grad(loss_t, model.parameters(), retain_graph=True, create_graph=True,allow_unused=True)\n",
    "        \n",
    "        loss_v = TE.calc_loss_batch(model,loss_criterion_1,validation_dataloader)\n",
    "        grads_v = torch.autograd.grad(loss_v, model.parameters(), retain_graph=True, create_graph=True,allow_unused=True)\n",
    "        \n",
    "        \n",
    "        utils.ignore_layers(model,['main_layer','classifier_layer'], reset = True)\n",
    "        loss = loss_fn(grads_t,grads_v)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        utils.unfreeze_params(model)\n",
    "        model.train()\n",
    "        \n",
    "    print(f'Epoch = {epoch}, main_loss = {loss_t}, grad_loss = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    print(n)\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(grads_t,grads_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
